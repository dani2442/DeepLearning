#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\date{}
\usepackage[colorlinks=true, urlcolor=blue]{hyperref}
\usepackage{cancel}
\setlength{\fboxrule}{1mm} 
\setlength{\fboxsep}{6mm}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2.25cm
\topmargin 2.5cm
\rightmargin 2.25cm
\bottommargin 2.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip smallskip
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\noindent
\align center

\series bold
\size larger
Least-Squares Regression
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
In least-squares regression, the training data contains 
\begin_inset Formula $n$
\end_inset

 different training pairs 
\begin_inset Formula $(\overline{X_{1}},y_{1})\dots(\overline{X_{n}},y_{n})$
\end_inset

, where each 
\begin_inset Formula $\overline{X_{i}}$
\end_inset

 is a 
\begin_inset Formula $d$
\end_inset

-dimesnional representation of the data points, and each 
\begin_inset Formula $y_{i}$
\end_inset

 is a real-valued target.
 The fact that the target is 
\shape italic
real-valued
\shape default
 is important, because the underlying problem is then referred to as 
\shape italic
regression
\shape default
 rather than 
\shape italic
classification
\shape default
.
 In fact, as we will see later, one can also use least-squares regression
 on binary targets by “pretending” that these targets are real-valued.
 The resulting approach is equivalent to the Widrow-Hoff learning algorithm,
 which is famous in the neural network literature as the second learning
 algorithm proposed after the perceptron.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
In least-squares regression, the target variable is related to the feature
 variables using the relationship 
\begin_inset Formula $\hat{y}_{i}=\overline{W}\cdot\overline{X_{i}}$
\end_inset

.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
The portion of the loss that is specific to the 
\begin_inset Formula $i$
\end_inset

th training instance is given by the following 
\begin_inset Formula $L_{i}=e_{i}^{2}=(y_{i}-\hat{y}_{i})^{2}$
\end_inset

.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
The stochastic gradient-descent steps are determined by computing the gradient
 of 
\begin_inset Formula $e_{i}^{2}$
\end_inset

 with respect to 
\begin_inset Formula $\overline{W}$
\end_inset

, when the training pair 
\begin_inset Formula $(\overline{X_{i}},y_{i})$
\end_inset

 is presented to the neural network.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\bullet$
\end_inset

 This gradient can be computed as follows 
\begin_inset Formula $\frac{\partial{L_{i}}}{\partial{\overline{W}}}=-e_{i}\overline{X_{i}}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula $\Rightarrow L_{i}=y_{i}^{2}+\hat{y}_{i}^{2}-2y_{i}\hat{y}_{i}=y_{i}^{2}+(\overline{W}\cdot\overline{X_{i}})^{2}-2y_{i}(\overline{W}\cdot\overline{X_{i}})\Rightarrow$
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Formula $\Rightarrow\frac{\partial{L_{i}}}{\partial{\overline{W}}}=2(\overline{W}\cdot\overline{X_{i}})\cdot\overline{X_{i}}-2y_{i}\overline{X_{i}}=2(\hat{y}_{i}-y_{i})\overline{X_{i}}=-2e_{i}\overline{X_{i}}$
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\bullet$
\end_inset

 Therefore, the gradient-descent updates for 
\begin_inset Formula $\overline{W}$
\end_inset

 are computed using the above gradient and step-size 
\begin_inset Formula $\alpha$
\end_inset

 (hyperparameter), 
\begin_inset Formula $\overline{W}\Leftarrow\overline{W}+\alpha e_{i}\overline{X_{i}}=\overline{W}+\alpha(y_{i}-\hat{y}_{i})\overline{X_{i}}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula $\Rightarrow\overline{W}\Leftarrow\overline{W}+2\alpha e_{i}\overline{X_{i}}$
\end_inset

 so we could to not take into account the 2 if we think that 
\begin_inset Formula $\alpha\Leftarrow2\alpha$
\end_inset

.
\end_layout

\begin_layout Standard
With regularization, the update is as follows 
\begin_inset Formula $\overline{W}\Leftarrow\overline{W}(1-\alpha\lambda)+\alpha(y_{i}-\hat{y}_{i})\overline{X_{i}}$
\end_inset

, where 
\begin_inset Formula $\lambda>0$
\end_inset

 is the regularization parameter.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
What if we applied least-squares regression directly to minimize the squared
 distance of the real-valued prediction 
\begin_inset Formula $\hat{y}_{i}$
\end_inset

 from the observed binary targets 
\begin_inset Formula $y_{i}\in\{-1,+1\}$
\end_inset

? The direct application of least-squares regression to binary targets is
 referred to as least-squares classification.
 The gradient-descent is the same as the one shown above.
\end_layout

\begin_layout Standard
This direct application of least-squares regression to binary targets is
 referred to as Widrow-Hoff learning.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard

\series bold
\size large
Widrow-Hoff Learning
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
The loss function of the Widrow-Hoff method can be rewritten slightly from
 least-squares regression because of its binary responses, when working
 with binary responses in 
\begin_inset Formula $\{-1,+1\}$
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\Rightarrow L_{i}=(y_{i}-\hat{y}_{i})^{2}=y_{i}^{2}(y_{i}-\hat{y}_{i})^{2}=(y_{i}^{2}-y_{i}\hat{y}_{i})^{2}=(1-y_{i}\hat{y}_{i})^{2}$
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
One of the flows of this method is that it penalizes over-performance, and
 other methods can be shown to be closely related the Widrow-Hoff loss function
 by using different ways of repairing the loss so that over-performance
 is not penalized.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
The gradient-descent updates of least-squares regresion can be rewritten
 slightly for Widrow-Hoff learning becuase of binary response variables:
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Formula $\overline{W}\Leftarrow\overline{W}(1-\alpha\cdot\lambda)+\alpha(y_{i}-\hat{y}_{i})\overline{X_{i}}=\overline{W}(1-\alpha\cdot\lambda)+\alpha y_{i}(1-y_{i}\hat{y}_{i})\overline{X_{i}}$
\end_inset


\end_layout

\begin_layout Standard

\series bold
\size large
Closed Form Solutions
\size default

\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
The special case of least-squares regression and classification is solvable
 in closed form (without gradient-descent) by using the pseudo-inverse of
 the 
\begin_inset Formula $n\times d$
\end_inset

 training data matrix 
\begin_inset Formula $D$
\end_inset

, whose rows are 
\begin_inset Formula $\overline{X_{1}},\dots,\overline{X_{n}}$
\end_inset

.
 Let the $n$-dimensional column vector of dependent variables be denoted
 by 
\begin_inset Formula $\overline{y}=[y_{1},\dots,y_{n}]^{T}$
\end_inset

 .
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\bullet$
\end_inset

 The pseudo-inverse of matrix 
\begin_inset Formula $D$
\end_inset

 is defined as 
\begin_inset Formula $D^{+}=(D^{T}D)^{-1}D^{T}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula $\bullet$
\end_inset

 Then, the row-vector 
\begin_inset Formula $\overline{W}$
\end_inset

 is defined by 
\begin_inset Formula $\overline{W}^{T}=D^{+}\overline{y}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula $\bullet$
\end_inset

 If regularization is incorporated, 
\begin_inset Formula $\overline{W}^{T}=(D^{T}D+\lambda I)^{-1}D^{T}\overline{y}$
\end_inset

.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
One rarely inverts large matrices like 
\begin_inset Formula $D^{T}D$
\end_inset

.
 In fact, the Widrow-Hoff updates provide a very efficient way of solving
 the problem without using the closed form solution.
\end_layout

\end_body
\end_document

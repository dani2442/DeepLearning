{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2GznWWZ5Jzs8Kz5WKd89j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dani2442/DeepLearning/blob/master/untitled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taFLvwE98U7Z"
      },
      "source": [
        "## **Least-Squares Regression**\n",
        "\n",
        "In least-squares regression, the training data contains $n$ different training pairs $(\\overline{X_1},y_1)\\dots(\\overline{X_n},y_n)$, where each $\\overline{X_i}$ is a $d$-dimesnional representation of the data points, and each $y_i$\n",
        "is a real-valued target. The fact that the target is *real-valued* is important, because the underlying problem is then referred to as *regression* rather than *classification*. In fact, as we will see later, one can also use least-squares regression on binary targets by “pretending” that these targets are real-valued. The resulting approach is equivalent to the Widrow-Hoff\n",
        "learning algorithm, which is famous in the neural network literature as the second learning algorithm proposed after the perceptron.\n",
        "\n",
        " In least-squares regression, the target variable is related to the feature variables using the relationship $\\hat{y}_i=\\overline{W}\\cdot\\overline{X_i}$.\n",
        "\n",
        "The portion of the loss that is specific to the $i$th training instance is given by the following $L_i=e_i^2=(y_i-\\hat{y}_i)^2$.\n",
        "\n",
        "The stochastic gradient-descent steps are determined by computing the gradient of $e_i^2$ with respect to $\\overline{W}$, when the training pair $(\\overline{X_i},y_i)$ is presented to the neural network. \n",
        "\n",
        "* This gradient can be computed as follows $\\frac{\\partial{L_i}}{\\partial{\\overline{W}}}=-e_i \\overline{X_i}$.\n",
        "\n",
        "$\\Rightarrow L_i=y_i^2+\\hat{y}_i^2-2y_i\\hat{y}_i=y_i^2+(\\overline{W}\\cdot\\overline{X_i})^2-2y_i(\\overline{W}\\cdot\\overline{X_i}) \\Rightarrow  $\n",
        "\n",
        "$\\Rightarrow \\frac{\\partial{L_i}}{\\partial{\\overline{W}}}=2(\\overline{W}\\cdot\\overline{X_i})\\cdot \\overline{X_i}-2y_i\\overline{X_i}=2(\\hat{y}_i-y_i)\\overline{X_i}=-2e_i\\overline{X_i}$\n",
        "\n",
        "* Therefore, the gradient-descent updates for $\\overline{W}$ are computed using the above gradient and step-size $\\alpha$ (hyperparameter), $\\overline{W}\\Leftarrow \\overline{W} + \\alpha e_i \\overline{X_i}=\\overline{W} + \\alpha (y_i-\\hat{y}_i) \\overline{X_i}$.\n",
        "\n",
        "$\\Rightarrow \\overline{W}\\Leftarrow \\overline{W} + 2\\alpha e_i \\overline{X_i}$  so we could to not take into account the 2 if we think that $\\alpha\\Leftarrow 2\\alpha$.\n",
        "\n",
        "With regularization, the update is as follows $\\overline{W}\\Leftarrow \\overline{W}(1-\\alpha\\lambda) + \\alpha (y_i-\\hat{y}_i) \\overline{X_i}$, where $\\lambda >0$ is the regularization parameter.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpOLJzzZBOdG"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# starting with a list I=[[X_1,y_1],...,[X_n,y_n]] where X_i=[X_i1,...,X_id].\n",
        "\n",
        "W_0 = np.random.rand(1,d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh9L1PFQOipG"
      },
      "source": [
        "What if we applied least-squares regression directly to minimize the squared distance of the real-valued prediction $\\hat{y}_i$ from the observed binary targets $y_i \\in {-1,+1}$? The direct application of least-squares regression to binary targets is referred to as least-squares classification. The gradient-descent is the same as the one shown above.\n",
        "\n",
        "This direct application of least-squares regression to binary targets is referred to as Widrow-Hoff learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYSB2X0nPaHx"
      },
      "source": [
        "### **Widrow-Hoff Learning**\n",
        "\n",
        "The loss function of the Widrow-Hoff method can be rewritten slightly from least-squares regression because of its binary responses, when working with binary responses in {$-1,+1$}\n",
        "\n",
        "$\\Rightarrow L_i = (y_i-\\hat{y}_i)^2=y_i^2(y_i-\\hat{y}_i)^2=(y_i^2-y_i\\hat{y}_i)^2=(1-y_i\\hat{y}_i)^2$\n",
        "\n",
        "One of the flows of this method is that it penalizes over-performance, and other methods can be shown to be closely related the Widrow-Hoff loss function by using different ways of repairing the loss so that over-performance is not penalized.\n",
        "\n",
        "The gradient-descent updates of least-squares regresion can be rewritten slightly for Widrow-Hoff learning becuase of binary response variables:\n",
        "\n",
        "$\\overline{W} \\Leftarrow \\overline{W}(1-\\alpha \\cdot \\lambda) + \\alpha (y_i-\\hat{y}_i)\\overline{X_i}=\\overline{W}(1-\\alpha \\cdot \\lambda) + \\alpha y_i(1-y_i\\hat{y}_i)\\overline{X_i}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw8JG0JYTuF3"
      },
      "source": [
        "### **Closed Form Solutions**\n",
        "\n",
        "The special case of least-squares regression and classification is solvable in closed form (without gradient-descent) by using the pseudo-inverse of the $n \\times d$ training data matrix $D$, whose rows are $\\overline{X_1},\\dots,\\overline{X_n}$. Let the $n$-dimensional column vector of dependent variables\n",
        "be denoted by $\\overline{y} = [y_1,\\dots,y_n]^T$ . \n",
        "\n",
        "The pseudo-inverse of matrix $D$ is defined as $D^+ = (D^T D)^{-1} D^T$\n",
        "\n",
        "Then, the row-vector $\\overline{W}$ is defined by $\\overline{W}^T = D^+ \\overline{y}$\n",
        "\n",
        "If regularization is incorporated, $\\overline{W}^T = (D^T D +\\lambda I)^{-1}D^T \\overline{y}$\n",
        "\n",
        "One rarely inverts large matrices like $D^T D$. In fact, the Widrow-Hoff updates provide a very efficient way of solving the problem without using the closed form solution."
      ]
    }
  ]
}